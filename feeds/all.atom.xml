<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>富贵闲人</title><link href="orangeprince.info/" rel="alternate"></link><link href="orangeprince.info/feeds/all.atom.xml" rel="self"></link><id>orangeprince.info/</id><updated>2014-04-16T00:00:00+08:00</updated><entry><title>libsvm与liblinear</title><link href="orangeprince.info/libsvmyu-liblinear.html" rel="alternate"></link><updated>2014-04-16T00:00:00+08:00</updated><author><name>orangeprince</name></author><id>tag:,2014-04-16:orangeprince.info/libsvmyu-liblinear.html</id><summary type="html">&lt;p&gt;记得有一次在北京参加一个学习班，其中一个主讲是CMU的Eric Xing老师。讲座结束之后照例是观众提问环节，其中有一个观众向Eric Xing提问：“你们使用SVM一般用哪一个工具包？”Eric Xing老师显然对于这样的问题不屑一顾，他回答道在他们CMU，从来不用别人写的package。他们自己有一个machine learning的库，所以类似SVM这样的算法都是自己用C++自己实现的。当时想想，这位提问同学真是有点自讨没趣。&lt;/p&gt;
&lt;!--more--&gt;

&lt;p&gt;不过现在看来，其实也这位同学的提问其实也是很有意义的，只不过提问的对象可能不合适。从学习的角度，我强烈建议能够自己实现一些基础的learning算法，比如说Logistic Regression和SVM这类。只有真正自己实现过一些算法，才能对于这些算法的本质有更深入的理解（说到这里，我自己也很惭愧）。但是很多算法其实有一些细节是自己实现时不能完全cover到的。就拿SVM来说，看起来目标函数是一个形式比较简单的二次优化问题，但是对于大kernel矩阵的优化和SMO算法中对于样本点的选取，都是有很多学问的。有经验的算法开发者可以通过对这些细节问题的精益求精大大提升算法的效率和对资源的占用，而这些细节往往是初学者实现算法时难以注意到的。如果这个时候使用一个非常完善的工具包，在效率和准确度上都有保障，又有非常方便的调用接口，那么自己的工作显然可以更加有效率，而且可以把更多的精力放在关注问题本身，而不是琐碎的算法实现上。事实上，每个人的精力总归是有限的，CMU可能拥有大量算法和coding都很强的学生，研究方向是machine learning的lab也确实有必要自己从底层实现这些算法，可是对于很多其他的人来说，对于这些算法的需求可能仅仅是应用。毕竟，对于细节的追求可惜无限精细下去，如果嫌其他的SVM实现不好，可以自己实现一套，可是如果觉得用现有的编程语言实现不好，是不是也可以自己创造一门语言，如果觉得现有的CPU和体系结构也不满意，甚至对现在的计算理论也不满意呢？人的精力总是有限的，我想最重要的是要把自己有限的精力放在对自己来说最重要的事情上面。从这个角度上来说，使用一些非常成熟好用的工具包，确实是一个很有效率的办法。              &lt;/p&gt;
&lt;p&gt;说了这么多废话，终于进入正题，推荐一些自己觉得好用的工具。首先要推荐的工具是大名鼎鼎的libsvm。SVM是机器学习中应用最广的算法之一，而在libsvm恐怕又是各种机器SVM开源库中的翘楚。libsvm是台湾大学林智仁(Lin Chih-Jen)副教授团队开发的，这个团队还开发了很多其他耳熟能详的机器学习工具包，可以说是品质的保证。&lt;/p&gt;
&lt;h2&gt;目标问题&lt;/h2&gt;
&lt;p&gt;首先看看libsvm要解决的问题。libsvm要解的是一个标准SVM对偶优化问题，也就是下面的形式：
$$
\begin{aligned}
\underset{\mathbf{\alpha}}{\operatorname{argmin}} \quad &amp;amp; f(\mathbf{\alpha}) =
\frac{1}{2} \mathbf{\alpha}^T Q \mathbf{\alpha} - e^T \mathbf{\alpha} \\
subject\,to \quad &amp;amp; 0 \le \alpha_i \le C, i= 1,\ldots,l, \\
&amp;amp; \mathbf{y}^T \mathbf{\alpha}= 0
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;稍微解释一下: \(\alpha\) 是需要优化的变量，每一个 \(\alpha_i\) 对应的就是第 
\(i\) 个sample在分类器中的权重。实际上 \(\alpha\) 是非常稀疏的，大部分的样本点对于分类器并没有实质的贡献，少数点被选择成为支持向量。&lt;/p&gt;
&lt;p&gt;得到\( \alpha \),就可以方便对新的数据点进行分类：
$$
y = sgn \left( \sum_{i} \alpha_i y_i \mathcal{K}(x, x_i) + b \right)
$$&lt;/p&gt;
&lt;p&gt;求解上面目标函数的一个经典算法叫做SMO(Sequential minimal optimization），在这里不讨论算法的细节。libsvm实现了这一优化算法，并且对算法进行了的优化，大大提高了计算效率，详情可参考介绍libsvm算法实现的&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf"&gt;论文&lt;/a&gt;。此外libsvm还支持nu-SVC, SVM regression, one-class SVM等SVM的变种，考虑到使用场景不多，这里暂时不详细介绍。&lt;/p&gt;
&lt;p&gt;总结来说，libsvm的优化过程无论在算法上还是code上都经过千锤百炼，效率肯定高于一些简单的SMO实现。当然，libsvm是直接对svm的对偶问题进行优化，主要面向的是带kernel的SVM。对于更简单的线性SVM问题，其实有更好的解决方案，例如liblinear。&lt;/p&gt;
&lt;h2&gt;使用方法&lt;/h2&gt;
&lt;h3&gt;数据格式&lt;/h3&gt;
&lt;p&gt;libsvm对于训练数据和测试数据的格式有着特定的要求，具体格式如下：
{% highlight python %}
&lt;label&gt; &lt;index1&gt;:&lt;value1&gt; &lt;index2&gt;:&lt;value2&gt;
{% endhighlight %}
每一行代表一个训练样本及其对应的标签。&lt;/p&gt;
&lt;p&gt;其中的&lt;code&gt;&amp;lt;label&amp;gt;&lt;/code&gt;表示用户指定的标签，或者说是对类别的一个标识，理论上可以是数字也可以是其他的字符串。这里有一个细节特别需要注意，libsvm系统内部对类别的编号与用户指定的label并不一致。比如在训练文件中指定为-1的标签，有可能在libsvm内部使用正样本来进行标识的。用户标签与内部类别编号的对应关系在libsvm的model文件中有专门的存储，在模型与测试需要转换。当需要人工读取训练完成的模型文件进行prediction时，一定要注意这个细节。最后，也有一个特例，对于二分类问题，如果训练文件的label只包含“+1”和“-1”两个标签，系统会默认也将标有“+1”的数据作为正样本，标有“-1”的数据作为负样本。&lt;/p&gt;
&lt;p&gt;相对而言，&lt;code&gt;&amp;lt;index&amp;gt;&lt;/code&gt;与&lt;code&gt;&amp;lt;value&amp;gt;&lt;/code&gt;的处理要简单一些。所有训练样本的特征从1开始按维度连续编号，对于非0的特征，则将对应的维度作为index，对应的特征值作为value。&lt;/p&gt;
&lt;h3&gt;模型训练&lt;/h3&gt;
&lt;p&gt;libsvm的训练需要注意一些参数的设置。比如&lt;code&gt;-s&lt;/code&gt;控制所使用的SVM模型，对于一般的分类任务来说，选择默认的C-SVM就可以解决问题。&lt;code&gt;-t&lt;/code&gt;设置不同的kernel函数。除了默认的linear kernel，libsvm还提供了常用的poly kernel, rbf kernel和sigmod kernel。libsvm也可以使用自定义的kernel函数，通过使用&lt;code&gt;-t 4&lt;/code&gt;参数。此时，输入的训练和测试数据的特征需要替换为训练数据计算的kernel矩阵和训练数据与测试数据计算的kernel矩阵。&lt;/p&gt;
&lt;p&gt;libsvm训练得到的模型结果默认用文本保存，主要的数据就是训练得到的支持向量和每个支持向量对应的\(\alpha)\。因为对于我们也可以简单的通过读取模型文件对新的数据进行分类。只需计算输入向量与每一个支持向量的kernel距离，再乘上系数\(\alpha)\即可，当然最后还需要加上bias项。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;libsvm是一个优秀的svm工具包。其最大的特点是训练效率高且使用灵活，比如可以方便使用自定义的kerenl函数。而且libsvm的可移植性非常的好，除了native的java移植外，还提供了python、R、matlab等各种常见语言的接口，并有一系列的扩展功能，如不同的评价指标，cross validation等。libsvm的局限性主要在于优化算法本身是面向复杂kernel函数的，对于情形更简单的linear kernel，则可以用更高效的liblinear包完成。&lt;/p&gt;
&lt;p&gt;除了libsvm外，&lt;a href="http://svmlight.joachims.org/"&gt;svmlight&lt;/a&gt;也是非常著名的svm工具包。除了提供基本的SVM算法外，svmlight支持struct SVM的扩展，这也是svmlight的一大优势。此外&lt;a href="http://vikas.sindhwani.org/svmlin.html"&gt;svmlin&lt;/a&gt;也是一个比较有特色的svm package，除了高效的线性svm实现外，svmlin对于半监督SVM有很好的支持，可以方便在标注数据较少时充分利用无标注数据进行模型的训练。&lt;/p&gt;
&lt;p&gt;最后，在我日常工作中最常用的工具包并不是libsvm而是&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/"&gt;liblinear&lt;/a&gt;。liblinear在处理线性SVM和Logistic Regression时有着更高的效率和与libsvm几乎同样的接口与可移植性。后面打算专门写文章来介绍liblinear的优化算法。而liblinear的使用方法和一些注意事项，在这个&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/faq"&gt;FAQ&lt;/a&gt;中已经有非常清楚的介绍，只要从头看到尾，基本上就能很好的使用liblinear。最近，liblinear还推出了分布式版本，分别支持MPI框架和Spark，详情可以参考此&lt;a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/"&gt;项目主页&lt;/a&gt;。&lt;/p&gt;</summary><category term="开发工具"></category><category term="SVM"></category></entry></feed>