---
layout: post
title: "比较深度学习和压缩感知"
category: "技术"
tags: [机器学习,知乎,深度学习]
---
原文URL：<http://www.zhihu.com/question/22836081/answer/30856233>

###问题

> 兄弟学习稀疏表示与压缩感知（CS）数年，博士论文也是这个方面的应用研究。不过最近一段看来，深度学习（DL）无疑是ML中最火的。查查历史，其实CS和DL的发端论文都在05、06年，CS在前几年也曾热极一时，但是DL却真正落了实。很想请懂行的大侠比较比较这两方面的研究，从理论性、实用性、未来前景上说说各自的优劣。还有就是CS和DL是否也有融合交叉的地方呢？谢谢哈！

<!--more-->

###回复

深度学习和压缩感知都是机器学习的一类思想方法，很多模型都是基于这些思想设计出来的。二者在基本思想上有一定的共通之处，最大的共同点是都强调特征的学习与构造，因而也出现了很多将两者结合的方法。但从根本上说，两者的适用场景和motivation还是有着较大的差别。

深度学习方法强调特征的逐层学习，虽然这种学习方式并没有从根本上提高模型的表达能力，但是却降低了学习的成本，同时也使得模型与人类理解的问题解决方法更加趋于一致。深度学习的思想既适用于监督学习的场景，也适用于无监督学习的场景。从原始特征输入开始，经过逐层的神经网络计算，最终得到的学习结果，既可以是分类的结果，也可以是提取出的高度抽象特征。而每一层的计算过程，都可以看作是提取特征的一个步骤，每一层计算的结果，都可以看成是中间层的特征表达。

压缩感知本质上是对输入的特征进行了基变换，形成了新的特征表达，所以也可以看成是特征学习的一种方法。但是与深度学习不同，压缩感知本身只是一个单层的算法。在给定了新的特征表达空间下的一组基向量（通常称为词典）后，对于新输入的特征，压缩感知需要从这些基向量中选择一部分向量，以完成对输入特征的重建。压缩感知主要的特点是在重建过程中要求选择的基向量尽量的少，也就是重建系数尽量的稀疏。这样做可以更好的对信号进行压缩，同时也在某种程度上提升了特征的表达能力，使得这个特征能够在一些任务(如分类)中取得更好的效果。

这样就很容易看出两者的区别：深度学习是一个类更加通用的方法，几乎可以解决传统机器学习中遇到的各种问题，强调模型的层次性。而压缩感知只是一个单层的无监督学习的方法，强调模型的稀疏性。

但另一个方面，两者又存在着很多联系。比如Staked Sparse Autoencoder直接把多个单层基于压缩感知的稀疏编码机叠加在一起形成了一个深度学习的模型。也就是说，压缩感知的思想可以非常方便的应用在深度学习的单层模型中，成为深度学习的一部分。其实不止于此，广泛用于图像识别的卷积神经网络中的pooling层其实也和压缩感知的思想有很多共同之处。从本质上来说，深度学习的特征提取都要经历一个从底层到高层，从具体到抽象的过程。在这个过程中，从不同角度或者来源得到的底层特征必然有一个汇总到高层特征的过程。在这个汇总的过程中，也必然要保留或构造出那些最有价值的特征，而丢弃掉那些价值较低的特征。而这个过程，恰恰是压缩感知可以发挥威力的地方，因为压缩感知的基本思想就是去伪存真，只保留信号中最有价值的那一部分信息。