---
layout: post-no-feature
title: "LIBSVM与LIBLINEAR（三）"
category: tech
tags: [MachineLearning]
---

## 调节参数
LIBSVM和LIBLINEAR工具包都包含很多需要调节的参数，参数的调节既需要足够的耐心，也有着很多的技巧。当然，还需要对参数本身的意义和对模型的影响了如指掌。下面主要讨论一些对模型影响较大的参数

### 参数C
参数$C$是在LIBLINEAR和LIBSVM的求解中都要用到的一个参数。前面说到的各种模型，可以写成统一的形式：

$$
\large
\begin{align}
\underset{w}{\operatorname{argmin}}  \quad \Omega(\phi(w))  + C \sum_{i=1}^l \ell(y_i, \phi(w)^T\phi(x_i))
\end{align}
$$

其中右边的一项是模型的损失项，其大小表明了分类器对样本的拟合程度。而左边的一项，则是人为加上的损失，与训练样本无关，被称作正则化项(Regularizer)，反映了对训练模型额外增加的一些约束。而参数$C$则负责调整两者之间的权重。$C$越大，则要求模型能够更好地拟合训练样本数据，反之，则要求模型更多的满足正则化项的约束。以LIBLINEAR为例，下面先讨论LIBLINEAR下$\ell_2$norm的情况：

$$
\large
\begin{align}
\underset{w}{\operatorname{argmin}}  \quad \parallel w \parallel_2^2  + C \sum_{i=1}^l \ell(y_i, w^Tx_i)
\end{align}
$$

之所以要增加正则化项，是因为在设计模型的时候，我们对于样本的质量以及模型的泛化能力没有充分的自信，认为在没有其他约束的情况下，训练得到的模型会因为过于迁就已有的样本数据而无法对新的数据达到同样的效果。在这个时候，就必须在模型中增加人类的一些经验知识。比如上面对$\phi(w)$增加$\ell_2$norm的约束就是如此。如果上面公式中的损失函数对应一个回归问题，那么这个问题就被称作Ridge Regression，中文叫做脊回归或者岭回归。

我们可以站在不同的角度来理解$\ell_2$norm正则化项的意义。如果把学习分类函数中$w$看作是一个参数估计的问题，那么不带正则化项的目标函数对应的就是对$w$进行最大似然估计的问题。为了使$w$的估计更加接近真实的情况，我们可以根据经验对$w$制定一个先验分布。当我们假设$w$先验分布是一个多元高斯分布，且不同维度之间是没有关联的(即协方差矩阵非对角线元素为$0$)，而每一个维度特征的方差为某一固定制，那么推导出来的最大后验概率就是上面的带正则化项的目标函数。而$C$与$w$先验分布的方差相关。$C$越大，就意味着正则化的效果偏弱，$w$的波动范围可以更大，先验的方差也更大；而$C$越小，则意味着正则化的效果更强，$w$的波动范围变小，先验的方差也变小。通过减小$C$的值，可以控制$w$的波动不至于过大，以免受一些数据的影响，造成模型的过拟合（overfitting）。　  
另外也有一种更直观的解释，上面regularized形式的目标函数也可以根据KKT条件转为constraint形式，也就是：

$$
\large
\begin{align}
\underset{w}{\operatorname{argmin}} \quad &  \sum_{i=1}^l \ell(y_i, w^Tx_i) \nonumber \\ 
s.t. \quad & \parallel w \parallel_2^2 < s^2
\end{align}
$$


通过参数$s$限制$w$的大小，而$s$与$C$也存在着一定正向相关的关系。因此，当$C$较小时，$w$的取值也被限制在的一个很小的范围内。下面的图给了一个非常直观的解释：

![L2Norm](/images/11/l2.svg)

由于有了对w取值的限制，就出现了两种情况。第一种是当$s$不够大的时候，此时如果沿梯度下降的方向一直搜索，找到全局最优解，就已经找出圈外，不满足下面的约束项。这个时候，只能在满足约束的条件下找到尽量好的解。根据KKT条件，此时的最优解一定是划定范围的圆圈与目标函数等梯度线相切的位置，如上图左边所示。如果把梯度图看成一座山的等高线，那边最优解的位置一定是等高线中凸起的部分，类似于一座山上的山脊或者山岭，这也是脊（岭）回归的由来。另一种情况是当$s$足够大的时候，这个时候，在由$s$所划定的范围内已经能够达到全局最优解，这个时候，下面的约束项其实并没有起到作用，就如上图右边所示。

因此在调参过程中，如果数据量较少，或者我们对数据的质量信心不足，就应该减少$C$的大小，增加先验的重要性，反之则可以增加$C$的大小，让数据本身起更大的作用。而在优化过程中，$C$越大，需要搜索的$w$的范围也越大，计算的代价也会越高。而从前面的分析中可以看出，当$C$增加到一定程度后，模型已经能确保达到全局最优，此时继续增加$C$对提高算法的表现已经没有帮助。因此，在LIBLINEAR中，实际推荐的做法是将$C$的值从小向大来调，当$C$增加之后已经无法改变算法的效果时，说明$C$对模型已经没有影响，就没有必要继续调下去了。选择较小的$C$反而可以提高模型收敛的速度。

### $\ell_1$norm的使用
在LIBLINEAR中，除了提供上面提到的$\ell_2$norm正则化项之外，还提供了$\ell_1$norm的选项。$\ell_1$norm一般写成$\parallel w \parallel_1$，其实就是对向量$w$中的所有元素的绝对值进行求和。与$\ell_2$norm相比，$\ell_1$norm也具有对$w$本身大小的约束，使得$w$的某些维度值不至于过大而导致过拟合。这个特性在统计学上也称作收缩”shrinkage”。而此外，$\ell_1$norm还有另外一个非常有用的特征，即能够使学习到的$w$比较稀疏（sparse），也就是存在很多的$0$项，而且可以通过系数$C$控制$0$项的的个数。当$C$减小时，$w$的非0项就增多，当$C$无限小时，由于完全没有拟合损失的压力，$w$也可以变成全部是$0$了。

为什么$\ell_1$norm还有这样的功能呢？说来话长，下次可以专门再写博客讨论了。简单来说，是由$\ell_1$norm的特殊性质决定的。$\ell_1$norm所对应的绝对值函数是连续的，但并非处处可导，因为在$0$点存在特殊的情况，即左右导数实不相等的。因此可以认为定义sub_gradient来描述这种特殊情况的导数，这样的结果是在这个特殊点上，导数不是一个值，而是左右倒数中间的一个范围。由于在$0$点导数取值非常灵活，使得在模型求解的过程中，很容易在这样的点达到极值，也就会使得学习到的$w$尽量的稀疏。

基于$\ell_1$的线性回归模型也被叫做LASSO。采用这个惩罚项的基本动机是认为只有少数特征是与分类结果真正相关，而绝大多数特征是无关的。这一假设确实存在一定的道理，但是在实际分类准确度上，$\ell_1$norm正则化项对$\ell_2$norm项并没有绝对的优势，这是因为$\ell_2$norm虽然不能直接去除那些与分类结果无关的特征，但是模型学习的结果也会让这些特征的权重很低，所以他们能起到的影响不大。但是$\ell_1$norm的一个好处是可以作为一个特征选择的工具，从高维特征空间中只选取少数一些与分类结果最为相关的特征进行计算。这在处理大量高维数据或者实时计算问题是，还是非常有帮助的，可以大大减少存储，提高计算的效率。

### Kernel相关参数
如果使用LIBSVM的话，参数调节的工作就会更复杂一些。首先是kernel的使用。一般来说rbf kernel是被鼓励优先使用的。如果使用rbf kernel效果都无法调到满意，那么采用poly或linear也无济于事。在一些特殊场景下，可以考虑自定义kernel，这在LIBSVM中也是支持的。

rbf的全称是Radial Basis Function，中文叫做径向基函数。而一般在SVM使用的rbf是Gaussian RBF，也就是我们一般所说的高斯核函数。给定两个点$x_1$和$x_2$，Gaussian RBF定义为：
		
$$
\begin{align} 
\large
K(x_1, x_2) = exp(-\gamma \parallel x_1 - x_2 \parallel^2)
\end{align}
$$

可见，高斯核函数是对两点之间的欧氏距离进行了一定的变换，且变换受到参数$\gamma$的控制。应该怎样理解高斯核函数的意义与$\gamma$的作用的？


$$
\begin{align}
\large
K(x_1, x_2) &= exp(-\gamma\parallel x_1 - x_2\parallel^2) \nonumber \\
			&= exp( - \gamma(-\parallel x_1 \parallel^2 + 2x_1^Tx_2  -\parallel x_2 \parallel^2))\nonumber \\
			&= exp( -\gamma \parallel x_1 \parallel ^2) exp(-\gamma \parallel x_2 \parallel ^2) exp(2\gamma x_1^Tx_2 )\nonumber \\
			&= exp( -\gamma \parallel x_1 \parallel ^2) exp(-\gamma \parallel x_2 \parallel ^2) \sum_{n=0}^\infty \frac{(2\gamma x_1^Tx_2)^n}{n!}
\end{align}
$$

当$\gamma$较小，或者说在极端情况趋向于$0$的时候，可以有$\sum_{n=0}^\infty \frac{(2\gamma x_1^Tx_2)^n}{n!} \approx 2\gamma x_1^Tx_2 $，也就是$n>1$以后的项远小于$n=1$项。这个时候，rbf kernel的效果其实和linear kernel相差无几。

相反，当$\gamma$增大时，$n>1$以后的项也产生作用，其基本思想和poly kernel差不多，只是rbf直接把维度从有限维度的多项式上升到了无穷维的多项式而已。当$\gamma$无限增大时，可以看到，除非$\parallel x_1 - x_2 \parallel^2$为$0$，否则K(x_1, x_2) 都会无限趋近于0。也就是说，当$\gamma$趋近于无穷大的时候，每一个数据点除了和其自身外，和其他点得距离都为$0$。在这种情况下，模型训练的结果只能是把所有点都作为支持向量，因此在训练数据上精度可以达到100%。这样的模型也可以看成KNN的特殊情况，此时的$K$等于$1$。

上面分别讨论了问题的两个极端情况。当$\gamma$无限小的时候，rbf核SVM和线性SVM效果类似，因此模型的复杂度，或者说VC维较低，不容易过拟合。而当$\gamma$值无限增大时，所有点都变成支持向量，模型复杂多或者说VC维最高，也最容易过拟合。而一般情况下，$\gamma$取一个中间值，也就间距两者的意义，相比于线性模型，可以选择更多的支持向量，增加模型的复杂度与拟合能力。而相比于1-NN模型，也会适当降低模型的复杂度，避免过拟合的风险。此外，从上面的讨论也可以看出，通过参数调整，rbf核基本上可以达到与线性核以及poly核差不多的效果。所以，在不考虑计算效率的情况下，为了达到最优模型，只需要针对rbf模型进行调参就可以了。

上面在参数调整的讨论里，都假设参数$C$是固定的。但在实际SVM的调参过程中，$C$和$\gamma$是同时变化的，这进一步增加了调参的复杂性。关于两个变量之间的关系，也有很多理论上的分析与讨论，这里不过多进行讨论，可以参考文件：Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel。

在实际的应用场景下，我们可以通过cross-validate的方法对参数进行遍历，选择最佳的参数组合。LIBSVM也提供了grid.py工具，用于SVM的调参。在一般的应用中，我们只需要设置参数的可变范围，然后在训练数据中对参数组合进行遍历，选择最优参数组合就可以了。

