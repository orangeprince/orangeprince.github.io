---
layout: post
title: "闲话SVM(一)"
category: "技术"
tags: [机器学习,SVM]
---

SVM，全称Support Vector Machines，中文叫做支持向量机，是最常用的机器学习算法之一。SVM的出现对机器学习的理论发展具有里程碑式的意义。而与此同时，SVM又是一个非常接地气的算法，模型的训练过程并不算复杂，而预测的过程则更加简单和优美。市面上存在着各种SVM的开源实现，而对于大部分的分类问题，SVM都可以得到比较好的分类效果，作为baseline，总是可以在各种paper中作为背景帝出现。

以前我对SVM的具体算法一直有畏惧的情绪。那是因为一次人工智能课的开卷考试，居然出了一道问答题让我回答什么是支持向量机（这里不得不黑一下母校当年的出卷老师，这里就不点名了）。我当时一个学期没怎么上过课，考前也没有看过书，对于SVM一无所知，只记得当时抄了整整一面卷子的公式，只觉天昏地暗。从此，心理就对SVM就产生了一种畏惧的情绪。直到若干年以后，我才慢慢领悟到，原来SVM并没有这么难，其实是可以用很简单的话就能够说清楚的。然而，后来我发现我又错了，但这是后话了。这里，我尝试先抛开各种繁琐的公式与理论，简单的说说我的SVM的直观理解。

<!--more-->

机器学习中的分类问题，看起来其实并不是太复杂。最直接的做法，是把已经标好的正负样本点都放在它们所在的线性空间进行观察。需要学习的分类函数则是这个空间里的一个平面或者曲面，能够很好地把训练集中的正负样本分隔开来，当然，更重要的是，还需要对训练数据以外的数据同样起作用。为了描述简单，我们就假设所有的样本都是二维空间中的一个点，而针对这样样本的线性分类器，则是在平面上的一条直线。

下面的图展示了一个成功的线性分类器，它成功地分隔了训练数据集中的正样本和负样本（分别对应图中的原点和方点）。这个场景就像刚刚摆好棋子的象棋盘，双方棋子各在楚河汉界一边，泾渭分明。当然，这样的情况往往只在理想中，很少出现在现实世界，否则，也不用去花这么多的力气研究各种各样的分类算法了。不过，这样一个简单的例子，可以作为理解SVM的一个开始。
![svm示意图]({{ site.img_url }}/svm/svm1.svg)

前面给出了分类器的直观几何解释，下面来用代数的语言简单的对这个分类的问题进行一下描述。对于上面空间中的每一个点，可以用一个向量\\( x_i\\)来表示。最后分类的结果，可以用\\( (+1,-1)\\)表示，分别对应正样本和负样本。而我们学习的分类器则可以看成是一个这样的函数\\(f\\)，它的输入是\\(x\\)，输入\\( f(x)\\)则是\\(+1\\)或\\(-1\\)中的某一个值。显然，这个分类函数与上图中的分类平面有着非常密切的联系。

考虑上面图中的情况，假设这个分类直线的方程是  $$  x_1w_1 + x_2w_2 + b = 0 $$，正样本在直线的上方，负样本在直线的下方，当$$ x_1w_1 + x_2w_2 + b > 0 $$时，点$$x$$应该处在直线上方，是正样本，而当$$ x_1w_1 + x_2w_2 + b < 0 $$时，$$x$$则被这条分类直线看做是负样本。当维度升高时，之前的二维分类直线变成了高维空间中的超平面，上面的情况依然是适用的。那么参照超平面的方程，就可以写出线性分类器情况下分类函数$$f$$的一种形式：
$$
\begin{aligned}
f(x) = sgn(\sum_{i=1}^{n}x_iw_i) + b
\end{aligned}
$$
其中$$n$$表示特征空间的维度，sgn是取符号的函数啦。