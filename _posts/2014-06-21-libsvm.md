---
layout: post
title: "一些好用的机器学习工具---libsvm与liblinear"
category: "技术"
tags: [机器学习,工具]
---

记得有一次在北京参加一个学习班，其中一个主讲是CMU的Eric Xing老师。讲座结束之后照例是观众提问环节，其中有一个观众向Eric Xing提问：“你们使用SVM一般用哪一个工具包？”Eric Xing老师显然对于这样的问题不屑一顾，他回答道在他们CMU，从来不用别人写的package。他们自己有一个machine learning的库，所以类似SVM这样的算法都是自己用C++自己实现的。当时想想，这位提问同学真是有点自讨没趣。
   
不过现在看来，其实也这位同学的提问其实也是很有意义的，只不过提问的对象可能不合适。从学习的角度，我强烈建议能够自己实现一些基础的learning算法，比如说Logistic Regression和SVM这类。只有真正自己实现过一些算法，才能对于这些算法的本质有更深入的理解（说到这里，我自己也很惭愧）。但是很多算法其实有一些细节是自己实现时不能完全cover到的。就拿SVM来说，看起来目标函数是一个形式比较简单的二次优化问题，但是对于大kernel矩阵的优化和SMO算法中对于样本点的选取，都是有很多学问的。有经验的算法开发者可以通过对这些细节问题的精益求精大大提升算法的效率和对资源的占用，而这些细节往往是初学者实现算法时难以注意到的。如果这个时候使用一个非常完善的工具包，在效率和准确度上都有保障，又有非常方便的调用接口，那么自己的工作显然可以更加有效率，而且可以把更多的精力放在关注问题本身，而不是琐碎的算法实现上。事实上，每个人的精力总归是有限的，CMU可能拥有大量算法和coding都很强的学生，研究方向是machine learning的lab也确实有必要自己从底层实现这些算法，可是对于很多其他的人来说，对于这些算法的需求可能仅仅是应用。毕竟，对于细节的追求可惜无限精细下去，如果嫌其他的SVM实现不好，可以自己实现一套，可是如果觉得用现有的编程语言实现不好，是不是也可以自己创造一门语言，如果觉得现有的CPU和体系结构也不满意，甚至对现在的计算理论也不满意呢？人的精力总是有限的，我想最重要的是要把自己有限的精力放在对自己来说最重要的事情上面。从这个角度上来说，使用一些非常成熟好用的工具包，确实是一个很有效率的办法。    	        

说了这么多废话，终于进入正题，推荐一些自己觉得好用的工具。首先要推荐的工具是大名鼎鼎的libsvm。SVM是机器学习中应用最广的算法之一，而在libsvm恐怕又是各种机器SVM开源库中的翘楚。libsvm是台湾大学林智仁(Lin Chih-Jen)副教授团队开发的，这个团队还开发了很多其他耳熟能详的机器学习工具包，基本上凡是他们出品，必属精品。

首先看看libsvm要解决的问题。libsvm要解的是一个标准SVM对偶优化问题，也就是下面的形式：
$$
\begin{aligned}
\underset{\mathbf{\alpha}}{\operatorname{argmin}} \quad & f(\mathbf{\alpha}) =
\frac{1}{2} \mathbf{\alpha}^T Q \mathbf{\alpha} - e^T \mathbf{\alpha} \\\
subject\,to \quad & 0 \le \alpha_i \le C, i= 1,\ldots,l, \\\
& \mathbf{y}^T \mathbf{\alpha}= 0
\end{aligned}
$$

稍微解释一下: \\(\alpha\\) 是需要优化的变量，每一个 \\(\alpha_i\\) 对应的就是第 
\\(i\\) 个sample在分类器中的权重。实际上 \\(\alpha\\) 是非常稀疏的，大部分的样本点对于分类器并没有实质的贡献，少数点被选择成为support vectors。

得到\\( \alpha \\),就可以方便对新的数据点进行分类：
$$
y = sgn\large\sum_{i} \alpha_i y_i \mathcal{K}(x, x_i) + b\large
$$

这里不去讲具体的优化算法了。