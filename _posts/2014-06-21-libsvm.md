---
layout: post
title: "一些好用的机器学习工具---libsvm"
category: "技术"
tags: [机器学习,工具]
---

记得有一次在北京参加一个学习班，其中一个主讲是CMU的Eric Xing老师。讲座结束之后照例是观众提问环节，其中有一个观众向Eric Xing提问：“你们使用SVM一般用哪一个工具包？”Eric Xing老师显然对于这样的问题不屑一顾，他回答道在他们CMU，从来不用别人写的package。他们自己有一个machine learning的库，所以类似SVM这样的算法都是自己用C++自己实现的。当时想想，这位提问同学真是有点自讨没趣。
   
不过现在看来，其实也这位同学的提问其实也是很有意义的，只不过提问的对象可能不合适。从学习的角度，我强烈建议能够自己实现一些基础的learning算法，比如说Logistic Regression和SVM这类。只有真正自己实现过一些算法，才能对于这些算法的本质有更深入的理解（说到这里，我自己也很惭愧）。但是很多算法其实有一些细节是自己实现时不能完全cover到的。就拿SVM来说，看起来目标函数是一个形式比较简单的二次优化问题，但是对于大kernel矩阵的优化和SMO算法中对于样本点的选取，都是有很多学问的。有经验的算法开发者可以通过对这些细节问题的精益求精大大提升算法的效率和对资源的占用，而这些细节往往是初学者实现算法时难以注意到的。如果这个时候使用一个非常完善的工具包，在效率和准确度上都有保障，又有非常方便的调用接口，那么自己的工作显然可以更加有效率，而且可以把更多的精力放在关注问题本身，而不是琐碎的算法实现上。事实上，每个人的精力总归是有限的，CMU可能拥有大量算法和coding都很强的学生，研究方向是machine learning的lab也确实有必要自己从底层实现这些算法，可是对于很多其他的人来说，对于这些算法的需求可能仅仅是应用。毕竟，对于细节的追求可惜无限精细下去，如果嫌其他的SVM实现不好，可以自己实现一套，可是如果觉得用现有的编程语言实现不好，是不是也可以自己创造一门语言，如果觉得现有的CPU和体系结构也不满意，甚至对现在的计算理论也不满意呢？人的精力总是有限的，我想最重要的是要把自己有限的精力放在对自己来说最重要的事情上面。从这个角度上来说，使用一些非常成熟好用的工具包，确实是一个很有效率的办法。    	        

说了这么多废话，终于进入正题，推荐一些自己觉得好用的工具。首先要推荐的工具是大名鼎鼎的libsvm。SVM是机器学习中应用最广的算法之一，而在libsvm恐怕又是各种机器SVM开源库中的翘楚。libsvm是台湾大学林智仁(Lin Chih-Jen)副教授团队开发的，这个团队还开发了很多其他耳熟能详的机器学习工具包，基本上他们出品，必属精品。

##目标问题

首先看看libsvm要解决的问题。libsvm要解的是一个标准SVM对偶优化问题，也就是下面的形式：
$$
\begin{aligned}
\underset{\mathbf{\alpha}}{\operatorname{argmin}} \quad & f(\mathbf{\alpha}) =
\frac{1}{2} \mathbf{\alpha}^T Q \mathbf{\alpha} - e^T \mathbf{\alpha} \\\
subject\,to \quad & 0 \le \alpha_i \le C, i= 1,\ldots,l, \\\
& \mathbf{y}^T \mathbf{\alpha}= 0
\end{aligned}
$$

稍微解释一下: \\(\alpha\\) 是需要优化的变量，每一个 \\(\alpha_i\\) 对应的就是第 
\\(i\\) 个sample在分类器中的权重。实际上 \\(\alpha\\) 是非常稀疏的，大部分的样本点对于分类器并没有实质的贡献，少数点被选择成为支持向量。

得到\\( \alpha \\),就可以方便对新的数据点进行分类：
$$
y = sgn \left( \sum_{i} \alpha_i y_i \mathcal{K}(x, x_i) + b \right)
$$

求解上面目标函数的一个经典算法叫做SMO(Sequential minimal optimization），在这里不讨论算法的细节。libsvm实现了这一优化算法，并且对算法进行了的优化，大大提高了计算效率，详情可参考介绍libsvm算法实现的[论文](http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf)。此外libsvm还支持nu-SVC, SVM regression, one-class SVM等SVM的变种，考虑到使用场景不多，这里暂时不详细介绍。

总结来说，libsvm的优化过程无论在算法上还是coding上都经过千锤百炼，效率肯定高于一些简单的SMO实现。当然，libsvm是直接对svm的对偶问题进行优化，主要面向的是带kernel的SVM。对于更简单的线性SVM问题，其实有更好的解决方案，例如liblinear。

##使用方法
 
###数据格式
libsvm对于训练数据和测试数据的格式有着特定的要求，具体格式如下：
{% highlight python %}
<label> <index1>:<value1> <index2>:<value2>
{% endhighlight %}
每一行代表一个训练样本及其对应的标签。

其中的`<label>`表示用户指定的标签，或者说是对类别的一个标识，理论上可以是数字也可以是其他的字符串。这里有一个细节特别需要注意，libsvm系统内部对类别的编号与用户指定的label并不一致。比如在训练文件中指定为-1的标签，有可能在libsvm内部使用正样本来进行标识的。用户标签与内部类别编号的对应关系在libsvm的model文件中有专门的存储，在模型与测试需要转换。当需要人工读取训练完成的模型文件进行prediction时，一定要注意这个细节。最后，也有一个特例，对于二分类问题，如果训练文件的label只包含“+1”和“-1”两个标签，系统会默认也将标有“+1”的数据作为正样本，标有“-1”的数据作为负样本。

相对而言，`<index>`与`<value>`的处理要简单一些。所有训练样本的特征从1开始按维度连续编号，对于非0的特征，则将对应的维度作为index，对应的特征值作为value。

###模型训练
